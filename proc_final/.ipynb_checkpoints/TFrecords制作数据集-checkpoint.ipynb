{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiyumei/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import threading\n",
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import glob\n",
    "import anno_func \n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据集\n",
    "current=os.getcwd()\n",
    "DATADIR = os.path.join(current,'../dataset/data')\n",
    "IMG_PATH=DATADIR+'/test'\n",
    "MARK_PATH=DATADIR+'/marks' \n",
    "# 位置标记文件\n",
    "filedir=DATADIR+'/annotations.json'\n",
    "# 每张图片名称所关联的id\n",
    "ids=open(DATADIR+'/test/ids.txt').read().splitlines()\n",
    "anns=json.loads(open(filedir).read())\n",
    "# print('anns',anns['types'])\n",
    "# 为类别生成数字编号\n",
    "targets=anns['types']\n",
    "target_classes=dict()\n",
    "for i,name in enumerate(targets):\n",
    "    target_classes[name]=i+1\n",
    "#     当读取不到数据，为背景图\n",
    "target_classes['None']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num(img_path):  \n",
    "    '''根据图片的名字截取后面的数字'''\n",
    "    return str(img_path.split('/')[-1].split('.')[0])\n",
    "\n",
    "def get_img_info(img):\n",
    "    '''读取单张图片的信息'''\n",
    "    imgid=img.split('.')[0]\n",
    "    imgdata = anno_func.load_img(anns, DATADIR, imgid)\n",
    "    imgdata_draw,mask_ellipse,img = anno_func.draw_all(anns, DATADIR, imgid, imgdata)\n",
    "    img_info=img['objects']\n",
    "    label_classes=[]\n",
    "    label_text=[]\n",
    "    bbox_list=[]\n",
    "    ymin = []\n",
    "    xmin = []\n",
    "    ymax = []\n",
    "    xmax = []\n",
    "    for obj in range(len(img_info)):\n",
    "        label_text.append(img_info[obj]['category'].encode())\n",
    "        bbox=img_info[obj]['bbox']\n",
    "        assert len(bbox) == 4\n",
    "        bbox_list.append([bbox['xmin'],bbox['ymin'],bbox['xmax'],bbox['ymax']])\n",
    "        label_classes.append(int(target_classes[img_info[obj]['category']]))\n",
    "    return bbox_list,label_classes,label_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不知道什么用途\n",
    "RANDOM_SEED = 180428\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"将int64类型加入到实例原型的装饰器\"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"将float类型加入到实例原型的装饰器\"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def _bytes_list_feature(value):\n",
    "    \"\"\"将二进制列表类型加入到实例原型的装饰器\"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"将二进制类型加入到实例原型的装饰器\"\"\"\n",
    "    if isinstance(value, six.string_types):\n",
    "        value = six.binary_type(value, encoding='utf-8')\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _convert_to_example(filename,image_name, image_buffer,bboxes,labels, labels_text,\n",
    "                        height, width):\n",
    "    \"\"\"\n",
    "    构建一个示例proto作为示例。\n",
    "    args：\n",
    "     filename：string，图像文件的路径，例如'/path/to/example.JPG'\n",
    "     image_buffer：字符串，RGB图像的JPEG编码\n",
    "     bboxes：每个图像的边界框列表\n",
    "     labels：边界框的标签列表\n",
    "     labels_text：边界框的标签名称列表\n",
    "     difficult：整数列表表明该边界框的难度\n",
    "     truncated：整数列表表示该边界框的截断\n",
    "     height：整数，图像高度（以像素为单位）\n",
    "     width：整数，图像宽度（以像素为单位）\n",
    "     Returns：\n",
    "     Example proto    \n",
    "    \"\"\"\n",
    "    ymin = []\n",
    "    xmin = []\n",
    "    ymax = []\n",
    "    xmax = []\n",
    "    for b in bboxes:\n",
    "        assert len(b) == 4\n",
    "        # pylint: disable=expression-not-assigned\n",
    "        [l.append(point) for l, point in zip([ymin, xmin, ymax, xmax], b)]\n",
    "    channels = 3\n",
    "    image_format = 'JPEG'\n",
    "#     循环获取信息，加入list\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'image/height': _int64_feature(height),\n",
    "            'image/width': _int64_feature(width),\n",
    "            'image/channels': _int64_feature(channels),\n",
    "            'image/shape': _int64_feature([height, width, channels]),\n",
    "            'image/object/bbox/xmin': _float_feature(xmin),\n",
    "            'image/object/bbox/xmax': _float_feature(xmax),\n",
    "            'image/object/bbox/ymin': _float_feature(ymin),\n",
    "            'image/object/bbox/ymax': _float_feature(ymax),\n",
    "            'image/object/bbox/label': _int64_feature(labels),\n",
    "            'image/object/bbox/label_text': _bytes_list_feature(labels_text),\n",
    "            'image/format': _bytes_feature(image_format),\n",
    "            'image/filename': _bytes_feature(filename.encode('utf8')),\n",
    "            'image/encoded': _bytes_feature(image_buffer)}))\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCoder(object):\n",
    "    \"\"\"Helper class that provides TensorFlow image coding utilities.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # sess 回调所有的图片\n",
    "        self._sess = tf.Session()\n",
    "\n",
    "        # 将png转成jpg.\n",
    "        self._png_data = tf.placeholder(dtype=tf.string)\n",
    "        image = tf.image.decode_png(self._png_data, channels=3)\n",
    "        self._png_to_jpeg = tf.image.encode_jpeg(image, format='rgb', quality=100)\n",
    "\n",
    "        # 将CMYK JPEG 转成 RGB JPEG d\n",
    "        self._cmyk_data = tf.placeholder(dtype=tf.string)\n",
    "        image = tf.image.decode_jpeg(self._cmyk_data, channels=0)\n",
    "        self._cmyk_to_rgb = tf.image.encode_jpeg(image, format='rgb', quality=100)\n",
    "\n",
    "        # 解码RGB\n",
    "        self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n",
    "        self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n",
    "\n",
    "    def png_to_jpeg(self, image_data):\n",
    "        return self._sess.run(self._png_to_jpeg,\n",
    "                          feed_dict={self._png_data: image_data})\n",
    "\n",
    "    def cmyk_to_rgb(self, image_data):\n",
    "        return self._sess.run(self._cmyk_to_rgb,\n",
    "                          feed_dict={self._cmyk_data: image_data})\n",
    "\n",
    "    def decode_jpeg(self, image_data):\n",
    "        image = self._sess.run(self._decode_jpeg,\n",
    "                               feed_dict={self._decode_jpeg_data: image_data})\n",
    "        assert len(image.shape) == 3\n",
    "        assert image.shape[2] == 3\n",
    "        return image\n",
    "\n",
    "\n",
    "def _process_image(filename, coder):\n",
    "    \"\"\"处理单张图片\n",
    "    Args:\n",
    "    filename: '/path/to/example.JPG'.\n",
    "    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n",
    "    Returns:\n",
    "    image_buffer: string, JPEG encoding of RGB image.\n",
    "    height: integer, image height in pixels.\n",
    "    width: integer, image width in pixels.\n",
    "    \"\"\"\n",
    "    # 读取图片.\n",
    "    with tf.gfile.FastGFile(filename, 'rb') as f:\n",
    "        image_data = f.read()\n",
    "\n",
    "    # 解码RGB JPEG.\n",
    "    image = coder.decode_jpeg(image_data)\n",
    "\n",
    "    # 转成 RGB\n",
    "    assert len(image.shape) == 3\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "#     print('image.shape[2]:',image.shape[2])\n",
    "    assert image.shape[2] == 3\n",
    "\n",
    "    return image_data, height, width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_image_files_batch(coder, thread_index, ranges, name, directory, all_records, num_shards):\n",
    "    \"\"\"在单线程中处理并保存为 TFRecord 图片列表\n",
    "    Args:\n",
    "    coder: ImageCoder实例提供TensorFlow图像编码工具。\n",
    "    thread_index: integer, unique batch to run index is within [0, len(ranges)).\n",
    "    ranges: 指定要并行分析的每个批次范围的整数对列表。\n",
    "    name: string,指定数据集的唯一标识符\n",
    "    directory: string; 所有数据的路径\n",
    "    all_records: 字符串元组列表; 每个元组的第一个是记录的子目录，第二个是图像文件名。\n",
    "    num_shards: num_shards：此数据集的整数分片数。\n",
    "    \"\"\"\n",
    "    # Each thread produces N shards where N = int(num_shards / num_threads).\n",
    "    # For instance, if num_shards = 128, and the num_threads = 2, then the first\n",
    "    # thread would produce shards [0, 64).\n",
    "    num_threads = len(ranges)\n",
    "    assert not num_shards % num_threads\n",
    "    num_shards_per_batch = int(num_shards / num_threads)\n",
    "\n",
    "    shard_ranges = np.linspace(ranges[thread_index][0],\n",
    "                             ranges[thread_index][1],\n",
    "                             num_shards_per_batch + 1).astype(int)\n",
    "    num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n",
    "\n",
    "    counter = 0\n",
    "    for s in range(num_shards_per_batch):\n",
    "        #生成文件名的分片版本, e.g. 'train-00002-of-00010'\n",
    "        shard = thread_index * num_shards_per_batch + s\n",
    "        output_filename = '%s-%.5d-of-%.5d' % (name, shard, num_shards)\n",
    "        output_file = os.path.join(output_directory, output_filename)\n",
    "#         with open(output_file, 'wb'):\n",
    "        writer = tf.python_io.TFRecordWriter(output_file)\n",
    "        \n",
    "\n",
    "        shard_counter = 0\n",
    "        files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n",
    "    #     print('files_in_shard',files_in_shard)\n",
    "        for i in files_in_shard:\n",
    "            cur_record = all_records[i]\n",
    "            filename = os.path.join(directory, cur_record[0],  cur_record[1])\n",
    "\n",
    "            image_buffer, height, width = _process_image(filename, coder)\n",
    "            bboxes,labels,labels_text=get_img_info(cur_record[1])\n",
    "            example = _convert_to_example(filename, cur_record[1], image_buffer, bboxes, labels, labels_text,\n",
    "                                        height, width)\n",
    "            writer.write(example.SerializeToString())\n",
    "#             print(height,width,shard_counter,example)\n",
    "            shard_counter += 1\n",
    "            counter += 1\n",
    "\n",
    "            if not counter % 1000:\n",
    "                print('%s [thread %d]: Processed %d of %d images in thread batch.' %\n",
    "                  (datetime.now(), thread_index, counter, num_files_in_thread))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        writer.close()\n",
    "        print('%s [thread %d]: Wrote %d images to %s' %\n",
    "              (datetime.now(), thread_index, shard_counter, output_file))\n",
    "        sys.stdout.flush()\n",
    "        shard_counter = 0\n",
    "    print('%s [thread %d]: Wrote %d images to %d shards.' %\n",
    "            (datetime.now(), thread_index, counter, num_files_in_thread))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_image_files(name, directory, all_records, num_shards):\n",
    "    \"\"\"处理并保存图像列表作为示例protos的TFRecord。\n",
    "    Args:\n",
    "    name: string, unique identifier specifying the data set\n",
    "    directory: string; the path of all datas\n",
    "    all_records:字符串元组列表; 每个元组的第一个是记录的子目录，第二个是图像文件名。 \n",
    "    num_shards: integer number of shards for this data set.\n",
    "    \"\"\"\n",
    "    # Break all images into batches with a [ranges[i][0], ranges[i][1]].\n",
    "    spacing = np.linspace(0, len(all_records),num_threads + 1).astype(np.int)\n",
    "    ranges = []\n",
    "    threads = []\n",
    "    for i in range(len(spacing) - 1):\n",
    "        ranges.append([spacing[i], spacing[i + 1]])\n",
    "\n",
    "    # 为每个批次启动一个线程。\n",
    "    print('Launching %d threads for spacings: %s' % (num_threads, ranges))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # 创建一个监视所有线程何时完成的机制。\n",
    "    coord = tf.train.Coordinator()\n",
    "\n",
    "    # 创建一个基于TensorFlow的通用实用程序，用于转换所有图像编码。\n",
    "    coder = ImageCoder()\n",
    "\n",
    "    threads = []\n",
    "    for thread_index in range(len(ranges)):\n",
    "        args = (coder, thread_index, ranges, name, directory, all_records, num_shards)\n",
    "        t = threading.Thread(target=_process_image_files_batch, args=args)\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    # Wait for all the threads to terminate.\n",
    "    coord.join(threads)\n",
    "    print('%s: Finished writing all %d images in data set.' %\n",
    "        (datetime.now(), len(all_records)))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_dataset(name, directory, all_splits, num_shards):\n",
    "    \"\"\"处理完整的数据集并将其另存为TFRecord。\n",
    "    Args:\n",
    "    name: string, unique identifier specifying the data set.\n",
    "    directory: string, root path to the data set.\n",
    "    all_splits: list of strings, sub-path to the data set.\n",
    "    num_shards: integer number of shards for this data set.\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    for split in all_splits:\n",
    "        jpeg_file_path = os.path.join(directory, split)\n",
    "        images = tf.gfile.ListDirectory(jpeg_file_path)\n",
    "        jpegs = [im_name for im_name in images if im_name.strip()[-3:]=='jpg']\n",
    "        all_records.extend(list(zip([split] * len(jpegs), jpegs)))\n",
    "        \n",
    "    shuffled_index = list(range(len(all_records)))\n",
    "    random.seed(RANDOM_SEED)\n",
    "    random.shuffle(shuffled_index)\n",
    "    all_records = [all_records[i] for i in shuffled_index]\n",
    "    _process_image_files(name, directory, all_records, num_shards)\n",
    "\n",
    "def parse_comma_list(args):\n",
    "    return [s.strip() for s in args.split(',')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成Tfrecords数据集\n",
    "num_threads=4\n",
    "train_shards=8\n",
    "directory=DATADIR\n",
    "test='test'\n",
    "output_directory=DATADIR+'/Tfrecords'\n",
    "# _process_dataset('train',directory,parse_comma_list(test),train_shards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "# from preprocessing import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_split_examples(split_path, file_prefix='.tfrecord'):\n",
    "    # Count the total number of examples in all of these shard\n",
    "    num_samples = 0\n",
    "    tfrecords_to_count = tf.gfile.Glob(os.path.join(split_path, file_prefix))\n",
    "    opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n",
    "    for tfrecord_file in tfrecords_to_count:\n",
    "        for record in tf.python_io.tf_record_iterator(tfrecord_file):#, options = opts):\n",
    "            num_samples += 1\n",
    "    return num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 3071\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    output_directory=DATADIR+'/Tfrecords'\n",
    "    print('train:', count_split_examples(output_directory, 'train-?????-of-?????'))\n",
    "#     print('val:', count_split_examples('/media/rs/7A0EE8880EE83EAF/Detections/SSD/dataset/tfrecords', 'val-?????-of-?????'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
